# -*- coding: utf-8 -*-
"""M21MA209_task1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FocUcJLoJtI_R1IeC_72Tl26J6ObkFLE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import seaborn as sns

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/ML/mnist_test.csv')

df.tail()

df.shape

# To find null if any null value exist
df.isnull().sum()

# to find data type of each attribute.
df.info()

df.describe()

mean_excluding_first = df.drop('label', axis=1).mean()
max_mean = mean_excluding_first.max()
max_mean

data=df.drop('label',axis=1)

data1=data.values

#define the function for cosine similarity.

def Cosine_Similarity(x, means):
    # Calculate the dot product between x and mu_k
    dot_products = np.dot(x, means.T)

    # Calculate the norms of x and mu_k
    x_norm = np.linalg.norm(x)
    means_norm = np.linalg.norm(means, axis=1)

    # formula for cosine similarity.
    distances = 1 - dot_products / (x_norm * means_norm)
    return distances
# at the end of this function I will get the distance.

max_iterations=10000
tolerance=1e-4
def k_means(data, k):
    # Step 1: Initialize mu_k randomly
    mu = max_mean*np.random.rand(k, data.shape[1])

    t = 0
    while t < max_iterations:
        # Step 2: Using cosine symmetry as distance.
        distances = Cosine_Similarity(data, mu)
        labels = np.argmin(distances, axis=1)

        # Step 3: Updating mu
        new_mu = np.array([data[labels == i].mean(axis=0) for i in range(k)])

        # Check for convergence by usng tolerance=1e-4
        if np.all(np.abs(mu - new_mu) < tolerance):
            break

        mu = new_mu
        t += 1

    return labels, mu

k = 10  # Number of clusters
label_10, mu = k_means(data1, k)

print(label_10)

for j in range(30):
  cluster_indices = np.where(label_10 == j)[0]

  if len(cluster_indices) == 0:
        continue
  selected_indices = np.random.choice(cluster_indices, 30 , replace=False)


  fig, axes = plt.subplots(3, 10, figsize=(6, 2))
  for i, idx in enumerate(selected_indices):

    selected_idx = np.random.choice(cluster_indices, 1)[0]
    digit_data = data1[selected_idx]
    image_data = digit_data.reshape(28, 28)


    ax = axes[i // 10, i % 10]
    ax.clear()
    ax.imshow(image_data, cmap='gray', interpolation='nearest')
    ax.axis('off')

  plt.suptitle(f"Cluster {j+1}", fontsize=12)
  plt.subplots_adjust(top=0.90)
  plt.show()

k = 7  # Number of clusters
label_7, mu = k_means(data1, k)

print(label_7)

for j in range(30):
  cluster_indices = np.where(label_7 == j)[0]

  if len(cluster_indices) == 0:
        continue
  selected_indices = np.random.choice(cluster_indices, 30 , replace=False)


  fig, axes = plt.subplots(3, 10, figsize=(6, 2))
  for i, idx in enumerate(selected_indices):

    selected_idx = np.random.choice(cluster_indices, 1)[0]
    digit_data = data1[selected_idx]
    image_data = digit_data.reshape(28, 28)


    ax = axes[i // 10, i % 10]
    ax.clear()
    ax.imshow(image_data, cmap='gray', interpolation='nearest')
    ax.axis('off')

  plt.suptitle(f"Cluster {j+1}", fontsize=12)
  plt.subplots_adjust(top=0.90)
  plt.show()

k = 4  # Number of clusters
label_4, mu = k_means(data1, k)

print(label_4)

for j in range(30):
  cluster_indices = np.where(label_4 == j)[0]

  if len(cluster_indices) == 0:
        continue
  selected_indices = np.random.choice(cluster_indices, 30 , replace=False)


  fig, axes = plt.subplots(3, 10, figsize=(6, 2))
  for i, idx in enumerate(selected_indices):

    selected_idx = np.random.choice(cluster_indices, 1)[0]
    digit_data = data1[selected_idx]
    image_data = digit_data.reshape(28, 28)


    ax = axes[i // 10, i % 10]
    ax.clear()
    ax.imshow(image_data, cmap='gray', interpolation='nearest')
    ax.axis('off')

  plt.suptitle(f"Cluster {j+1}", fontsize=12)
  plt.subplots_adjust(top=0.90)
  plt.show()



